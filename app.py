import streamlit as st
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import os

# 1. í˜ì´ì§€ ì„¤ì • ë° API í‚¤ ë¡œë“œ
st.set_page_config(page_title="PDF ChatBot (Gemini 2.5)", layout="wide")
st.title("ğŸ“„ PDF RAG ì±—ë´‡ (Gemini 2.5 Flash)")

try:
    api_key = st.secrets["GEMINI_API_KEY"]
except KeyError:
    st.error("Streamlit Secretsì— 'GEMINI_API_KEY'ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

# 2. RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶• (ìºì‹± ì²˜ë¦¬ë¡œ ì†ë„ í–¥ìƒ)
@st.cache_resource
def setup_rag_chain(uploaded_file=None):
    # íŒŒì¼ ì €ì¥ (PyPDFLoaderëŠ” ê²½ë¡œê°€ í•„ìš”í•¨)
    temp_file_path = "temp_pdf_storage.pdf"
    
    if uploaded_file:
        with open(temp_file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
    elif os.path.exists("test.pdf"):
        temp_file_path = "test.pdf"
    else:
        return None

    # PDF ë¡œë“œ ë° ë¶„í• 
    loader = PyPDFLoader(temp_file_path)
    documents = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    texts = text_splitter.split_documents(documents)

    # ì„ë² ë”© ë° ë²¡í„° ì €ì¥ì†Œ ìƒì„± (Gemini ëª¨ë¸ ì‚¬ìš©)
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=api_key)
    vectorstore = FAISS.from_documents(texts, embeddings)

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •
    template = """ë‹¹ì‹ ì€ ì—…ë¡œë“œëœ ë¬¸ì„œì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ëŠ” ë¹„ì„œì…ë‹ˆë‹¤. 
    ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ë©´ "ë¬¸ì„œì—ì„œ í•´ë‹¹ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.
    ë§íˆ¬ëŠ” ì •ì¤‘í•˜ê³  ê°„ê²°í•˜ê²Œ í•˜ì„¸ìš”.

    Context: {context}
    Question: {question}
    Answer:"""
    
    QA_CHAIN_PROMPT = PromptTemplate.from_template(template)

    # Gemini 2.5 Flash ëª¨ë¸ ì„¤ì •
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        google_api_key=api_key,
        temperature=0.1
    )

    # QA ì²´ì¸ ìƒì„±
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(),
        chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
    )
    return qa_chain

# 3. ì‚¬ì´ë“œë°” - íŒŒì¼ ì—…ë¡œë“œ
with st.sidebar:
    st.header("ì„¤ì •")
    uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="pdf")
    if st.button("ë¬¸ì„œ í•™ìŠµ ì‹œì‘"):
        with st.spinner("ë¬¸ì„œë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            st.session_state.qa_chain = setup_rag_chain(uploaded_file)
            st.success("í•™ìŠµ ì™„ë£Œ!")

# 4. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
if "messages" not in st.session_state:
    st.session_state.messages = []

# ê¸°ì¡´ ë©”ì‹œì§€ ì¶œë ¥
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ì±„íŒ… ì…ë ¥ì°½
if prompt := st.chat_input("ë¬¸ì„œì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        if "qa_chain" in st.session_state and st.session_state.qa_chain:
            response = st.session_state.qa_chain.invoke(prompt)
            answer = response["result"]
            st.markdown(answer)
            st.session_state.messages.append({"role": "assistant", "content": answer})
        else:
            st.warning("ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  'ë¬¸ì„œ í•™ìŠµ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
